{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch, Dataset, ArrayDataset\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    " \n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    CropForegroundd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    SpatialPadd,\n",
    "    SpatialCropd,\n",
    "    ScaleIntensityd,\n",
    "    ShiftIntensityd,\n",
    "    ResizeD,\n",
    "    AdjustContrastd,\n",
    "    RandGaussianSharpend,\n",
    "    RandHistogramShiftd,\n",
    "    RandStdShiftIntensityd,\n",
    "    RandAdjustContrastd,\n",
    "    ToDeviced\n",
    "    \n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import Activations, AsDiscrete\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset\n",
    "from monai.utils import set_determinism\n",
    "import os\n",
    "import glob\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from monai.transforms import Transform,ThresholdIntensity, RandCropByPosNegLabeld,Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, NormalizeIntensityd, ResizeD,RandAffined, RandFlip\n",
    "from monai.transforms import (\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "\n",
    "    LoadImaged,\n",
    "\n",
    ")\n",
    "from monai.data.utils import pad_list_data_collate, decollate_batch\n",
    "import os\n",
    "from monai.transforms import Compose, LoadImaged, AddChanneld, ScaleIntensityRanged, ToTensord\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =  \"/home/Task07_Pancreas/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "val_files, train_files = data_dicts[:5], data_dicts[5:15]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GettheDesiredLabels(Transform):\n",
    "    def __call__(self, data):\n",
    "        label = data[\"label\"]\n",
    "        # Keep only the second and third channels from the label\n",
    "        single_label = label[1:2]\n",
    "\n",
    "        data[\"label\"] = single_label\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        AsDiscreted(keys='label', to_onehot=3),\n",
    "        GettheDesiredLabels(),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        \n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        RandAffined(keys=[\"image\"],prob=0.5, translate_range=(10, 10, 5), rotate_range=(np.pi / 6, np.pi / 6, np.pi / 6), scale_range=(0.1, 0.1, 0.1)),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1, 1, 1), mode=(\"bilinear\", \"nearest\")),\n",
    "        ToDeviced(keys=[\"image\", \"label\"], device=\"cuda:0\"),\n",
    "        ResizeD(keys=[\"image\", \"label\"], spatial_size=[224, 224, 144], mode=(\"trilinear\", \"nearest\")),  # Resize image and label\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "        AsDiscreted(keys='label', to_onehot=3),\n",
    "        GettheDesiredLabels(),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"],\n",
    "            a_min=-57,\n",
    "            a_max=164,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "            clip=True,\n",
    "        ),\n",
    "        \n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(keys=[\"image\", \"label\"], pixdim=(1, 1, 1), mode=(\"bilinear\", \"nearest\")),\n",
    "        ResizeD(keys=[\"image\", \"label\"], spatial_size=[224, 224, 144], mode=(\"trilinear\", \"nearest\")),  # Resize image and label\n",
    "        ToTensord(keys=[\"image\", \"label\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 10/10 [00:08<00:00,  1.12it/s]\n",
      "Loading dataset: 100%|██████████| 5/5 [00:14<00:00,  2.83s/it]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CacheDataset(data=train_files, transform=train_transforms)\n",
    "val_dataset = CacheDataset(data=val_files, transform=val_transforms)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=pad_list_data_collate)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, collate_fn=pad_list_data_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 10\n",
      "Number of validation samples: 5\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values: 2\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "sample_index = 0  # Specify the index of the sample to check\n",
    "sample = train_dataset[sample_index]\n",
    "binarized_label = sample['label']\n",
    "unique_values = torch.unique(binarized_label)\n",
    "count = len(unique_values)\n",
    "print(f\"Number of unique values: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 224, 224, 144])\n",
      "Label shape: torch.Size([1, 224, 224, 144])\n"
     ]
    }
   ],
   "source": [
    "# Get the first sample in the dataset\n",
    "first_sample = train_dataset[0]\n",
    "\n",
    "\n",
    "image_shape = first_sample['image'].shape\n",
    "label_shape = first_sample['label'].shape\n",
    "\n",
    "print(f\"Image shape: {image_shape}\")\n",
    "print(f\"Label shape: {label_shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# Get image and labels\u001b[39;00m\n\u001b[1;32m     12\u001b[0m image \u001b[39m=\u001b[39m val_data_example[\u001b[39m'\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m, :, :, slice_idx]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[0;32m---> 13\u001b[0m label3 \u001b[39m=\u001b[39m val_data_example[\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m1\u001b[39;49m, :, :, slice_idx]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     14\u001b[0m label2 \u001b[39m=\u001b[39m val_data_example[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m, :, :, slice_idx]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     16\u001b[0m \u001b[39m# Create a figure\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/meta_tensor.py:268\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 268\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    269\u001b[0m \u001b[39m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m#     return ret\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "colors = [(0, 0, 1, i) for i in np.linspace(0, 1, 100)]\n",
    "blue_cmap = mcolors.LinearSegmentedColormap.from_list('blue_alpha', colors)\n",
    "\n",
    "colors = [(1, 0, 0, i) for i in np.linspace(0, 1, 100)]\n",
    "red_cmap = mcolors.LinearSegmentedColormap.from_list('red_alpha',colors)\n",
    "for i in range(1,3):                                               \n",
    "    val_data_example=train_dataset[i]\n",
    "    # Select a slice to visualize (e.g., the middle slice)\n",
    "    slice_idx = val_data_example['image'].shape[3] // 2  # adjust to the depth dimension\n",
    "    print(slice_idx)\n",
    "    # Get image and labels\n",
    "    image = val_data_example['image'][0, :, :, slice_idx].detach().cpu()\n",
    "    label3 = val_data_example['label'][1, :, :, slice_idx].detach().cpu()\n",
    "    label2 = val_data_example['label'][0, :, :, slice_idx].detach().cpu()\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(image, cmap='gray')\n",
    "\n",
    "    # Overlay the labels\n",
    "    # We're using different colors and transparency (alpha) for each label\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"image + label\")\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.imshow(label2, cmap=blue_cmap, alpha=0.5)  # Use alpha for transparency\n",
    "    plt.imshow(label3, cmap=red_cmap, alpha=0.5)  # Change 'jet' to any colormap you prefer\n",
    "\n",
    "    # plt.axis('off')  # To not show axis\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 GPUs!\n",
      "Using 3 GPUs!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# V-Net implementation\n",
    "class VNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(VNet, self).__init__()\n",
    "        \n",
    "        # Contracting Path\n",
    "        self.encoder1 = DoubleConv(in_channels, 16)\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = DoubleConv(16, 32)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = DoubleConv(32, 64)\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = DoubleConv(64, 128)\n",
    "        self.pool4 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(128, 256)\n",
    "        \n",
    "        # Expanding Path\n",
    "        self.upconv4 = nn.ConvTranspose3d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(256, 128)\n",
    "        self.upconv3 = nn.ConvTranspose3d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(128, 64)\n",
    "        self.upconv2 = nn.ConvTranspose3d(64, 32, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(64, 32)\n",
    "        self.upconv1 = nn.ConvTranspose3d(32, 16, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(32, 16)\n",
    "        \n",
    "        # Output\n",
    "        self.output = nn.Conv3d(16, out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Contracting Path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "        \n",
    "        # Expanding Path\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.decoder4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        # Output\n",
    "        output = self.output(dec1)\n",
    "        output = self.sigmoid(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Double convolution block\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        \n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm3d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "# Define your VNet model as a function\n",
    "def vnet(in_channels, out_channels):\n",
    "    model = VNet(in_channels, out_channels)\n",
    "    return model\n",
    "\n",
    "# Rest of the code (with VNet)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "num_gpus = torch.cuda.device_count()\n",
    "\n",
    "device_ids = [0]  # Specify the GPU device IDs to be used\n",
    "print(\"Using\", num_gpus, \"GPUs!\")\n",
    "\n",
    "# Define your VNet model\n",
    "in_channels = 1\n",
    "out_channels = 1\n",
    "model = vnet(in_channels, out_channels)\n",
    "model = model.to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model, device_ids=device_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Rest of the code\n",
    "max_epochs = 10\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(224, 224, 144),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/10, train_loss: 0.9863, step time: 0.2054\n",
      "2/10, train_loss: 0.9892, step time: 0.1571\n",
      "3/10, train_loss: 0.9868, step time: 0.1563\n",
      "4/10, train_loss: 0.9898, step time: 0.1566\n",
      "5/10, train_loss: 0.9897, step time: 0.2257\n",
      "6/10, train_loss: 0.9902, step time: 0.1564\n",
      "7/10, train_loss: 0.9896, step time: 0.2080\n",
      "8/10, train_loss: 0.9787, step time: 0.1563\n",
      "9/10, train_loss: 0.9800, step time: 0.2428\n",
      "10/10, train_loss: 0.9820, step time: 0.1560\n",
      "epoch 1 average loss: 0.9862\n",
      "current epoch: 1 current mean dice: 0.0065 class1: 0.0065 \n",
      "best mean dice: 0.0065 at epoch: 1\n",
      "time consuming of epoch 1 is: 25.9335\n",
      "----------\n",
      "epoch 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     19\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfor\u001b[39;00m batch_data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     21\u001b[0m     step_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m     step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/dataset.py:107\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(index, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mSequence):\n\u001b[1;32m    105\u001b[0m     \u001b[39m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39mreturn\u001b[39;00m Subset(dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, indices\u001b[39m=\u001b[39mindex)\n\u001b[0;32m--> 107\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(index)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/dataset.py:921\u001b[0m, in \u001b[0;36mCacheDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_cache:\n\u001b[1;32m    920\u001b[0m                 data \u001b[39m=\u001b[39m deepcopy(data)\n\u001b[0;32m--> 921\u001b[0m         data \u001b[39m=\u001b[39m apply_transform(_transform, data)\n\u001b[1;32m    922\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:102\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)) \u001b[39mand\u001b[39;00m map_items:\n\u001b[1;32m    101\u001b[0m         \u001b[39mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items) \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m data]\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mreturn\u001b[39;00m _apply_transform(transform, data, unpack_items)\n\u001b[1;32m    103\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    104\u001b[0m     \u001b[39m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[39m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[39mif\u001b[39;00m MONAIEnvVars\u001b[39m.\u001b[39mdebug():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:66\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, parameters, unpack_parameters)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(parameters, \u001b[39mtuple\u001b[39m) \u001b[39mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     64\u001b[0m     \u001b[39mreturn\u001b[39;00m transform(\u001b[39m*\u001b[39mparameters)\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m transform(parameters)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/spatial/dictionary.py:416\u001b[0m, in \u001b[0;36mSpacingd.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    411\u001b[0m d: Dict \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(data)\n\u001b[1;32m    412\u001b[0m \u001b[39mfor\u001b[39;00m key, mode, padding_mode, align_corners, dtype, scale_extent \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_iterator(\n\u001b[1;32m    413\u001b[0m     d, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malign_corners, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_extent\n\u001b[1;32m    414\u001b[0m ):\n\u001b[1;32m    415\u001b[0m     \u001b[39m# resample array of each corresponding key\u001b[39;00m\n\u001b[0;32m--> 416\u001b[0m     d[key] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mspacing_transform(\n\u001b[1;32m    417\u001b[0m         data_array\u001b[39m=\u001b[39;49md[key],\n\u001b[1;32m    418\u001b[0m         mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    419\u001b[0m         padding_mode\u001b[39m=\u001b[39;49mpadding_mode,\n\u001b[1;32m    420\u001b[0m         align_corners\u001b[39m=\u001b[39;49malign_corners,\n\u001b[1;32m    421\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    422\u001b[0m         scale_extent\u001b[39m=\u001b[39;49mscale_extent,\n\u001b[1;32m    423\u001b[0m     )\n\u001b[1;32m    424\u001b[0m \u001b[39mreturn\u001b[39;00m d\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/utils/deprecate_utils.py:221\u001b[0m, in \u001b[0;36mdeprecated_arg.<locals>._decorator.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m is_deprecated:\n\u001b[1;32m    219\u001b[0m         warn_deprecated(argname, msg, warning_category)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/spatial/array.py:606\u001b[0m, in \u001b[0;36mSpacing.__call__\u001b[0;34m(self, data_array, affine, mode, padding_mode, align_corners, dtype, scale_extent, output_spatial_shape)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m# we don't want to track the nested transform otherwise two will be appended\u001b[39;00m\n\u001b[1;32m    605\u001b[0m actual_shape \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(output_shape) \u001b[39mif\u001b[39;00m output_spatial_shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m output_spatial_shape\n\u001b[0;32m--> 606\u001b[0m data_array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msp_resample(\n\u001b[1;32m    607\u001b[0m     data_array,\n\u001b[1;32m    608\u001b[0m     dst_affine\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mas_tensor(new_affine),\n\u001b[1;32m    609\u001b[0m     spatial_size\u001b[39m=\u001b[39;49mactual_shape,\n\u001b[1;32m    610\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m    611\u001b[0m     padding_mode\u001b[39m=\u001b[39;49mpadding_mode,\n\u001b[1;32m    612\u001b[0m     align_corners\u001b[39m=\u001b[39;49malign_corners,\n\u001b[1;32m    613\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    614\u001b[0m )\n\u001b[1;32m    615\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrecompute_affine \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data_array, MetaTensor):\n\u001b[1;32m    616\u001b[0m     data_array\u001b[39m.\u001b[39maffine \u001b[39m=\u001b[39m scale_affine(affine_, original_spatial_shape, actual_shape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/utils/deprecate_utils.py:221\u001b[0m, in \u001b[0;36mdeprecated_arg.<locals>._decorator.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m is_deprecated:\n\u001b[1;32m    219\u001b[0m         warn_deprecated(argname, msg, warning_category)\n\u001b[0;32m--> 221\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/spatial/array.py:315\u001b[0m, in \u001b[0;36mSpatialResample.__call__\u001b[0;34m(self, img, src_affine, dst_affine, spatial_size, mode, padding_mode, align_corners, dtype)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     affine_xform \u001b[39m=\u001b[39m AffineTransform(\n\u001b[1;32m    309\u001b[0m         normalized\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m         mode\u001b[39m=\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m         reverse_indexing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     img \u001b[39m=\u001b[39m affine_xform(img\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), theta\u001b[39m=\u001b[39;49mxform, spatial_size\u001b[39m=\u001b[39;49mspatial_size)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m additional_dims:\n\u001b[1;32m    317\u001b[0m     full_shape \u001b[39m=\u001b[39m (chns, \u001b[39m*\u001b[39mspatial_size, \u001b[39m*\u001b[39madditional_dims)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/networks/layers/spatial_transforms.py:576\u001b[0m, in \u001b[0;36mAffineTransform.forward\u001b[0;34m(self, src, theta, spatial_size)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    572\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maffine and image batch dimension must match, got affine=\u001b[39m\u001b[39m{\u001b[39;00mtheta\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m image=\u001b[39m\u001b[39m{\u001b[39;00msrc_size[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    575\u001b[0m grid \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39maffine_grid(theta\u001b[39m=\u001b[39mtheta[:, :sr], size\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(dst_size), align_corners\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malign_corners)\n\u001b[0;32m--> 576\u001b[0m dst \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mgrid_sample(\n\u001b[1;32m    577\u001b[0m     \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49msrc\u001b[39m.\u001b[39;49mcontiguous(),\n\u001b[1;32m    578\u001b[0m     grid\u001b[39m=\u001b[39;49mgrid,\n\u001b[1;32m    579\u001b[0m     mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    580\u001b[0m     padding_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_mode,\n\u001b[1;32m    581\u001b[0m     align_corners\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malign_corners,\n\u001b[1;32m    582\u001b[0m )\n\u001b[1;32m    583\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4206\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4106\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Given an :attr:`input` and a flow-field :attr:`grid`, computes the\u001b[39;00m\n\u001b[1;32m   4107\u001b[0m \u001b[39m``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\u001b[39;00m\n\u001b[1;32m   4108\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4203\u001b[0m \u001b[39m.. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\u001b[39;00m\n\u001b[1;32m   4204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4205\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, grid):\n\u001b[0;32m-> 4206\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4207\u001b[0m         grid_sample, (\u001b[39minput\u001b[39;49m, grid), \u001b[39minput\u001b[39;49m, grid, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49mpadding_mode, align_corners\u001b[39m=\u001b[39;49malign_corners\n\u001b[1;32m   4208\u001b[0m     )\n\u001b[1;32m   4209\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   4210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4211\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnn.functional.grid_sample(): expected mode to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4212\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, but got: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mode)\n\u001b[1;32m   4213\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/overrides.py:1551\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1545\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1546\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1547\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1549\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1550\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1551\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[1;32m   1553\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1554\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/meta_tensor.py:268\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 268\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    269\u001b[0m \u001b[39m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[39m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[39m#     return ret\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1296\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:4244\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4236\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   4237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4240\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4241\u001b[0m     )\n\u001b[1;32m   4242\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4244\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_class1 = []\n",
    "\n",
    "model_dir =  \"/home/model_codes/\"\n",
    "\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    " \n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, labels)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_dataset) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_class1 = metric_batch[0].item()\n",
    "            metric_values_class1.append(metric_class1)\n",
    "     \n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "           \n",
    "                \n",
    "     \n",
    "\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" class1: {metric_class1:.4f} \"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir =  \"/home/model_codes/\"\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(model_dir, \"best_metric_model_bg_v4.pth\"),\n",
    ")\n",
    "print(\"saved new best metric model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot epochs vs train loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_epochs + 1), epoch_loss_values, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Epochs vs. Train Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot epochs vs mean dice score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(val_interval, max_epochs + 1, val_interval), metric_values, label='Mean Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Dice Score')\n",
    "plt.title('Epochs vs. Mean Dice Score')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot epochs vs class 1, 2, and 3 dice scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(val_interval, max_epochs + 1, val_interval), metric_values_class1, label='Pancreas')\n",
    "plt.plot(range(val_interval, max_epochs + 1, val_interval), metric_values_class2, label='Tumor')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.title('Epochs vs. Dice Score (Pancreas and Tumor)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
